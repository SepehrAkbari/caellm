{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f43beef",
   "metadata": {},
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eae1f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159c310",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b33ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, world! This is, an example, of tokenizing text.\n",
      "Tokens: ['Hello', ',', 'world', '!', 'This', 'is', ',', 'an', 'example', ',', 'of', 'tokenizing', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Hello, world! This is, an example, of tokenizing text.\"\n",
    "print(f\"Original: {example_text}\")\n",
    "example_result = re.split(r'([,.!]|\\s)', example_text)\n",
    "example_result = [token for token in example_result if token.strip()]\n",
    "print(f\"Tokens: {example_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc01aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw text: 20479\n",
      "Number of tokens: 4690\n",
      "First 5 tokens: ['I', 'HAD', 'always', 'thought', 'Jack']\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"Length of raw text: {len(raw_text)}\")\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [token.strip() for token in preprocessed if token.strip()]\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(f\"First 5 tokens: {preprocessed[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db8f72",
   "metadata": {},
   "source": [
    "## Building Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0bd97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26843b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d9b9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tokens with IDs: [('flung', 451), ('square', 919), ('incense', 569), ('Among', 13), ('deploring', 326)]\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "print(f\"Random tokens with IDs: {random.sample(list(vocab.items()), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f128665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_v1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {token: integer for integer, token in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            token.strip() for token in preprocessed if token.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59642c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [53, 530, 667, 987, 67, 7, 38, 1077, 841, 9]\n",
      "Decoded text: I have mentioned that Mrs. Gisburn was rich ;\n",
      "Pipeline output: I have mentioned that Mrs. Gisburn was rich ;\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer_v1(vocab)\n",
    "\n",
    "text = \"I have mentioned that Mrs. Gisburn was rich;\" # sample text from source\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Encoded IDs: {ids}\")\n",
    "\n",
    "tokens = tokenizer.decode(ids)\n",
    "print(f\"Decoded text: {tokens}\")\n",
    "\n",
    "pipeline = tokenizer.decode(tokenizer.encode(text))\n",
    "print(f\"Pipeline output: {pipeline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08af66",
   "metadata": {},
   "source": [
    "## Adding Context Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ab9b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Token not found in vocabulary - 'Smith'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer_v1(vocab)\n",
    "\n",
    "example_text= \"I have mentioned that Mrs. Smith was rich\"\n",
    "\n",
    "try:\n",
    "    print(f\"Encoded IDs: {tokenizer.encode(example_text)}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Token not found in vocabulary - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7db567c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocabulary size: 1132\n",
      "Last 5 tokens in vocabulary: [('younger', 1127), ('your', 1128), ('yourself', 1129), ('<|endoftext|>', 1130), ('<|unk|>', 1131)]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"New vocabulary size: {len(vocab)}\")\n",
    "print(f\"Last 5 tokens in vocabulary: {list(vocab.items())[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b277291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_v2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {token: integer for integer, token in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2aa1e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [53, 530, 667, 987, 67, 7, 1131, 1077, 841]\n",
      "Decoded tokens: I have mentioned that Mrs. <|unk|> was rich\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer_v2(vocab)\n",
    "\n",
    "print(f\"Encoded IDs: {tokenizer.encode(example_text)}\")\n",
    "print(f\"Decoded tokens: {tokenizer.decode(tokenizer.encode(example_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01496dc2",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774eeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
